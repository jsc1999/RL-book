{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "dc7e3ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Mapping, Dict, Tuple, TypeVar, Callable, Iterator, Sequence\n",
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath(\"/Users/justincramer/Documents/Coding/CME241/RL-book/\"))\n",
    "import rl.monte_carlo as mc\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState, SimpleInventoryMDPCap\n",
    "from rl.approximate_dynamic_programming import (ValueFunctionApprox,\n",
    "                                                QValueFunctionApprox,\n",
    "                                                NTStateDistribution)\n",
    "from rl.dynamic_programming import policy_iteration_result\n",
    "from rl.distribution import Distribution, Choose, Gaussian\n",
    "from rl.function_approx import DNNSpec, AdamGradient, DNNApprox\n",
    "from rl.approximate_dynamic_programming import back_opt_vf_and_policy, \\\n",
    "    back_opt_qvf, ValueFunctionApprox, QValueFunctionApprox\n",
    "\n",
    "from rl.iterate import last\n",
    "from rl.markov_decision_process import MarkovDecisionProcess, FiniteMarkovDecisionProcess, Policy, \\\n",
    "    TransitionStep, NonTerminal\n",
    "from rl.policy import DeterministicPolicy, RandomPolicy, UniformPolicy, FiniteDeterministicPolicy\n",
    "import rl.markov_process as mp\n",
    "from rl.returns import returns\n",
    "from rl.function_approx import FunctionApprox, Tabular\n",
    "from rl.chapter7.asset_alloc_discrete import AssetAllocDiscrete\n",
    "import rl.td as td\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e722fa",
   "metadata": {},
   "source": [
    "## 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930c3ad0",
   "metadata": {},
   "source": [
    "#### Optimal VF and Policy for Simple Inventory MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3578287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.660960231637496,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.991900091403522,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.89485578163003,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.660960231637496}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple inventory MDP\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "        SimpleInventoryMDPCap(\n",
    "            capacity=user_capacity,\n",
    "            poisson_lambda=user_poisson_lambda,\n",
    "            holding_cost=user_holding_cost,\n",
    "            stockout_cost=user_stockout_cost\n",
    "        )\n",
    "    \n",
    "opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "    si_mdp,\n",
    "    gamma=user_gamma\n",
    ")\n",
    "    \n",
    "pprint(opt_vf_pi)\n",
    "print(opt_policy_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1812e5b4",
   "metadata": {},
   "source": [
    "#### Optimal VF and Policy for Asset Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a278728e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [183]\u001b[0m, in \u001b[0;36m<cell line: 48>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m init_wealth_distr: Gaussian \u001b[38;5;241m=\u001b[39m Gaussian(μ\u001b[38;5;241m=\u001b[39minit_wealth, σ\u001b[38;5;241m=\u001b[39minit_wealth_stdev)\n\u001b[1;32m     38\u001b[0m aad: AssetAllocDiscrete \u001b[38;5;241m=\u001b[39m AssetAllocDiscrete(\n\u001b[1;32m     39\u001b[0m     risky_return_distributions\u001b[38;5;241m=\u001b[39mrisky_ret,\n\u001b[1;32m     40\u001b[0m     riskless_returns\u001b[38;5;241m=\u001b[39mriskless_ret,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     initial_wealth_distribution\u001b[38;5;241m=\u001b[39minit_wealth_distr\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     48\u001b[0m it_qvf: Iterator[QValueFunctionApprox[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 49\u001b[0m         \u001b[43maad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_induction_qvf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m it_qvf: Iterator[QValueFunctionApprox[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m     52\u001b[0m         aad\u001b[38;5;241m.\u001b[39mbackward_induction_qvf()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackward Induction on Q-Value Function\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/chapter7/asset_alloc_discrete.py:128\u001b[0m, in \u001b[0;36mAssetAllocDiscrete.backward_induction_qvf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m num_state_samples: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[1;32m    126\u001b[0m error_tolerance: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-6\u001b[39m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mback_opt_qvf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmdp_f0_mu_triples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmdp_f0_mu_triples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mγ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_state_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_state_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_tolerance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_tolerance\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/approximate_dynamic_programming.py:316\u001b[0m, in \u001b[0;36mback_opt_qvf\u001b[0;34m(mdp_f0_mu_triples, γ, num_state_samples, error_tolerance)\u001b[0m\n\u001b[1;32m    309\u001b[0m         next_return: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    310\u001b[0m             qvf[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]((s1, a)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    311\u001b[0m             mdp_f0_mu_triples[horizon \u001b[38;5;241m-\u001b[39m i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactions(s1)\n\u001b[1;32m    312\u001b[0m         ) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s1, NonTerminal) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m r \u001b[38;5;241m+\u001b[39m γ \u001b[38;5;241m*\u001b[39m next_return\n\u001b[1;32m    315\u001b[0m     this_qvf \u001b[38;5;241m=\u001b[39m approx0\u001b[38;5;241m.\u001b[39msolve(\n\u001b[0;32m--> 316\u001b[0m         [((s, a), mdp\u001b[38;5;241m.\u001b[39mstep(s, a)\u001b[38;5;241m.\u001b[39mexpectation(return_))\n\u001b[1;32m    317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mu\u001b[38;5;241m.\u001b[39msample_n(num_state_samples) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m mdp\u001b[38;5;241m.\u001b[39mactions(s)],\n\u001b[1;32m    318\u001b[0m         error_tolerance\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     qvf\u001b[38;5;241m.\u001b[39mappend(this_qvf)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(qvf)\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/approximate_dynamic_programming.py:316\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    309\u001b[0m         next_return: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m    310\u001b[0m             qvf[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]((s1, a)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    311\u001b[0m             mdp_f0_mu_triples[horizon \u001b[38;5;241m-\u001b[39m i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactions(s1)\n\u001b[1;32m    312\u001b[0m         ) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s1, NonTerminal) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m r \u001b[38;5;241m+\u001b[39m γ \u001b[38;5;241m*\u001b[39m next_return\n\u001b[1;32m    315\u001b[0m     this_qvf \u001b[38;5;241m=\u001b[39m approx0\u001b[38;5;241m.\u001b[39msolve(\n\u001b[0;32m--> 316\u001b[0m         [((s, a), \u001b[43mmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpectation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    317\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m mu\u001b[38;5;241m.\u001b[39msample_n(num_state_samples) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m mdp\u001b[38;5;241m.\u001b[39mactions(s)],\n\u001b[1;32m    318\u001b[0m         error_tolerance\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     qvf\u001b[38;5;241m.\u001b[39mappend(this_qvf)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(qvf)\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/distribution.py:84\u001b[0m, in \u001b[0;36mSampledDistribution.expectation\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpectation\u001b[39m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m     f: Callable[[A], \u001b[38;5;28mfloat\u001b[39m]\n\u001b[1;32m     81\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124;03m'''Return a sampled approximation of the expectation of f(X) for some f.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpectation_samples\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpectation_samples\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/distribution.py:84\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpectation\u001b[39m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m     f: Callable[[A], \u001b[38;5;28mfloat\u001b[39m]\n\u001b[1;32m     81\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124;03m'''Return a sampled approximation of the expectation of f(X) for some f.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m     85\u001b[0m                \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpectation_samples)) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpectation_samples\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/approximate_dynamic_programming.py:309\u001b[0m, in \u001b[0;36mback_opt_qvf.<locals>.return_\u001b[0;34m(s_r, i)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreturn_\u001b[39m(s_r: Tuple[State[S], \u001b[38;5;28mfloat\u001b[39m], i\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    308\u001b[0m     s1, r \u001b[38;5;241m=\u001b[39m s_r\n\u001b[0;32m--> 309\u001b[0m     next_return: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqvf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmdp_f0_mu_triples\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhorizon\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s1, NonTerminal) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r \u001b[38;5;241m+\u001b[39m γ \u001b[38;5;241m*\u001b[39m next_return\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/approximate_dynamic_programming.py:310\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreturn_\u001b[39m(s_r: Tuple[State[S], \u001b[38;5;28mfloat\u001b[39m], i\u001b[38;5;241m=\u001b[39mi) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    308\u001b[0m     s1, r \u001b[38;5;241m=\u001b[39m s_r\n\u001b[1;32m    309\u001b[0m     next_return: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[0;32m--> 310\u001b[0m         \u001b[43mqvf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m\n\u001b[1;32m    311\u001b[0m         mdp_f0_mu_triples[horizon \u001b[38;5;241m-\u001b[39m i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mactions(s1)\n\u001b[1;32m    312\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s1, NonTerminal) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r \u001b[38;5;241m+\u001b[39m γ \u001b[38;5;241m*\u001b[39m next_return\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/function_approx.py:61\u001b[0m, in \u001b[0;36mFunctionApprox.__call__\u001b[0;34m(self, x_value)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_value: X) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_value\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/function_approx.py:733\u001b[0m, in \u001b[0;36mDNNApprox.evaluate\u001b[0;34m(self, x_values_seq)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_values_seq: Iterable[X]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_values_seq\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/function_approx.py:714\u001b[0m, in \u001b[0;36mDNNApprox.forward_propagation\u001b[0;34m(self, x_values_seq)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    704\u001b[0m     x_values_seq: Iterable[X]\n\u001b[1;32m    705\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence[np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;124;03m    :param x_values_seq: a n-length iterable of input points\u001b[39;00m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;124;03m    :return: list of length (L+2) where the first (L+1) values\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;124;03m             1-D array of length n)\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m     inp: np\u001b[38;5;241m.\u001b[39mndarray \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_values_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m     ret: List[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m [inp]\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/Coding/CME241/RL-book/rl/function_approx.py:698\u001b[0m, in \u001b[0;36mDNNApprox.get_feature_values\u001b[0;34m(self, x_values_seq)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_feature_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_values_seq: Iterable[X]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_functions\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_values_seq\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps: int = 4\n",
    "μ: float = 0.13\n",
    "σ: float = 0.2\n",
    "r: float = 0.07\n",
    "a: float = 1.0\n",
    "init_wealth: float = 1.0\n",
    "init_wealth_stdev: float = 0.1\n",
    "\n",
    "excess: float = μ - r\n",
    "var: float = σ * σ\n",
    "base_alloc: float = excess / (a * var)\n",
    "\n",
    "risky_ret: Sequence[Gaussian] = [Gaussian(μ=μ, σ=σ) for _ in range(steps)]\n",
    "riskless_ret: Sequence[float] = [r for _ in range(steps)]\n",
    "utility_function: Callable[[float], float] = lambda x: - np.exp(-a * x) / a\n",
    "alloc_choices: Sequence[float] = np.linspace(\n",
    "    2 / 3 * base_alloc,\n",
    "    4 / 3 * base_alloc,\n",
    "    11\n",
    ")\n",
    "feature_funcs: Sequence[Callable[[Tuple[float, float]], float]] = \\\n",
    "    [\n",
    "        lambda _: 1.,\n",
    "        lambda w_x: w_x[0],\n",
    "        lambda w_x: w_x[1],\n",
    "        lambda w_x: w_x[1] * w_x[1]\n",
    "    ]\n",
    "dnn: DNNSpec = DNNSpec(\n",
    "    neurons=[],\n",
    "    bias=False,\n",
    "    hidden_activation=lambda x: x,\n",
    "    hidden_activation_deriv=lambda y: np.ones_like(y),\n",
    "    output_activation=lambda x: - np.sign(a) * np.exp(-x),\n",
    "    output_activation_deriv=lambda y: -y\n",
    ")\n",
    "init_wealth_distr: Gaussian = Gaussian(μ=init_wealth, σ=init_wealth_stdev)\n",
    "\n",
    "aad: AssetAllocDiscrete = AssetAllocDiscrete(\n",
    "    risky_return_distributions=risky_ret,\n",
    "    riskless_returns=riskless_ret,\n",
    "    utility_func=utility_function,\n",
    "    risky_alloc_choices=alloc_choices,\n",
    "    feature_functions=feature_funcs,\n",
    "    dnn_spec=dnn,\n",
    "    initial_wealth_distribution=init_wealth_distr\n",
    ")\n",
    "\n",
    "it_qvf: Iterator[QValueFunctionApprox[float, float]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "it_qvf: Iterator[QValueFunctionApprox[float, float]] = \\\n",
    "        aad.backward_induction_qvf()\n",
    "\n",
    "print(\"Backward Induction on Q-Value Function\")\n",
    "print(\"--------------------------------------\")\n",
    "print()\n",
    "for t, q in enumerate(it_qvf):\n",
    "    print(f\"Time {t:d}\")\n",
    "    print()\n",
    "    opt_alloc: float = max(\n",
    "        ((q((NonTerminal(init_wealth), ac)), ac) for ac in alloc_choices),\n",
    "        key=itemgetter(0)\n",
    "    )[1]\n",
    "    val: float = max(q((NonTerminal(init_wealth), ac))\n",
    "                     for ac in alloc_choices)\n",
    "    print(f\"Opt Risky Allocation = {opt_alloc:.3f}, Opt Val = {val:.3f}\")\n",
    "    print(\"Optimal Weights below:\")\n",
    "    for wts in q.weights:\n",
    "        pprint(wts.weights)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133f66f",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "860b8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to get optimal value function and policy for optimal Q-value function\n",
    "def get_opt_val_policy(mdp, qvf):\n",
    "    \n",
    "    states = mdp.non_terminal_states\n",
    "    values = {}\n",
    "    policy_dict = {}\n",
    "    for s in states:\n",
    "        _, a = qvf.argmax((s, a) for a in mdp.actions(s))\n",
    "        val = qvf((s, a))\n",
    "        values[s] = val\n",
    "        policy_dict[s] = a\n",
    "    return values, FiniteDeterministicPolicy(policy_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a884e9f",
   "metadata": {},
   "source": [
    "#### Simple Inventory MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "35ad3fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform distribution over non-terminal states\n",
    "init_distr: NTStateDistribution[InventoryState] = Choose(si_mdp.non_terminal_states)\n",
    "approx_0: QValueFunctionApprox[InventoryState, int] = Tabular()\n",
    "num_episodes: int = 1000\n",
    "# Q-Value Function Approximation\n",
    "qvf = last(itertools.islice(mc.glie_mc_control(si_mdp, init_distr, approx_0, user_gamma, lambda k: 1 / k), \n",
    "                            num_episodes))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b731879a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.62597350682885,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -28.054374978569978,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.46623407438349,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.021325006751088,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.436113768855254,\n",
       "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.423106126098315},\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=0)): Do Action 2\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=1)): Do Action 1\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=2)): Do Action 0\n",
       " For State NonTerminal(state=InventoryState(on_hand=1, on_order=0)): Do Action 1\n",
       " For State NonTerminal(state=InventoryState(on_hand=1, on_order=1)): Do Action 0\n",
       " For State NonTerminal(state=InventoryState(on_hand=2, on_order=0)): Do Action 0)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_opt_val_policy(si_mdp, qvf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94999d80",
   "metadata": {},
   "source": [
    "#### Asset Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "7480a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = aad.get_mdp(0)\n",
    "init_distr = init_wealth_distr\n",
    "approx_0 = aad.get_qvf_func_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "89ad4d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes: int = 1000\n",
    "# Q-Value Function Approximation\n",
    "qvf = last(itertools.islice(mc.glie_mc_control(mdp, init_distr, approx_0, user_gamma, lambda k: 1 / k), \n",
    "                            num_episodes))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "33841369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNApprox(feature_functions=[<function AssetAllocDiscrete.get_qvf_func_approx.<locals>.this_f at 0x7fa5e8106e50>, <function AssetAllocDiscrete.get_qvf_func_approx.<locals>.this_f at 0x7fa5e8106f70>, <function AssetAllocDiscrete.get_qvf_func_approx.<locals>.this_f at 0x7fa5e8103040>, <function AssetAllocDiscrete.get_qvf_func_approx.<locals>.this_f at 0x7fa5e81030d0>], dnn_spec=DNNSpec(neurons=[], bias=False, hidden_activation=<function <lambda> at 0x7fa655cea790>, hidden_activation_deriv=<function <lambda> at 0x7fa655cea820>, output_activation=<function <lambda> at 0x7fa655cea700>, output_activation_deriv=<function <lambda> at 0x7fa655cea0d0>), regularization_coeff=0.0, weights=[Weights(adam_gradient=AdamGradient(learning_rate=0.1, decay1=0.9, decay2=0.999), time=0, weights=array([[-0.86178018, -1.11402238,  0.11125622,  1.01734087]]), adam_cache1=array([[0., 0., 0., 0.]]), adam_cache2=array([[0., 0., 0., 0.]]))])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(mc.glie_mc_control(mdp, init_distr, approx_0, user_gamma, lambda k: 1 / k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0dde6",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429ab46",
   "metadata": {},
   "source": [
    "#### Simple Inventory MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "1e878991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -24.310699840956065,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -17.257659229168535,\n",
       "  NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -17.487758758963512,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -18.218688806408593,\n",
       "  NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -18.50992058741949,\n",
       "  NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -19.68121790577847},\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=0)): Do Action 1\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=1)): Do Action 1\n",
       " For State NonTerminal(state=InventoryState(on_hand=0, on_order=2)): Do Action 0\n",
       " For State NonTerminal(state=InventoryState(on_hand=1, on_order=0)): Do Action 1\n",
       " For State NonTerminal(state=InventoryState(on_hand=1, on_order=1)): Do Action 0\n",
       " For State NonTerminal(state=InventoryState(on_hand=2, on_order=0)): Do Action 0)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uniform distribution over non-terminal states\n",
    "init_distr: NTStateDistribution[InventoryState] = Choose(si_mdp.non_terminal_states)\n",
    "approx_0: QValueFunctionApprox[InventoryState, int] = Tabular()\n",
    "num_episodes: int = 10000\n",
    "max_episodes_length: int = 100\n",
    "qvf = last(itertools.islice(td.q_learning(si_mdp, mc.epsilon_greedy_policy, init_distr, approx_0, user_gamma, \n",
    "                                          max_episodes_length), \n",
    "                            num_episodes))\n",
    "get_opt_val_policy(si_mdp, qvf)\n",
    "\n",
    "qvf = last(itertools.islice(td.glie_sarsa(si_mdp, init_distr, approx_0, user_gamma, lambda k: 1 / k,\n",
    "                                          max_episodes_length),\n",
    "                            num_episodes))\n",
    "get_opt_val_policy(si_mdp, qvf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
